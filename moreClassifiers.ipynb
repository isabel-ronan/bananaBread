{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b06c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, cross_validate, GridSearchCV\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, make_scorer, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f528609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS_CV = 5\n",
    "N_JOBS = 1  # prioritizing no leakage\n",
    "\n",
    "def set_random_states(random_state):\n",
    "    np.random.seed(random_state)\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_state)\n",
    "    random.seed(random_state)\n",
    "\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    return random_state\n",
    "\n",
    "random_state = set_random_states(1618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7a881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.read_csv(\"./data/allIngredients.csv\")\n",
    "overall_df = overall_df.fillna(0)\n",
    "overall_df = overall_df.sample(frac=1, random_state=random_state)\n",
    "bread_cake_df = overall_df[overall_df['label'] != \"banana\"]\n",
    "banana_df = overall_df[overall_df['label'] == \"banana\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262261d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bread_cake_df.drop(\"label\", axis=1)\n",
    "y = bread_cake_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c3017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bc13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get models - broad coverage across linear, margin, instance-based, and nonlinear tree ensembles\n",
    "def build_models(random_state):\n",
    "    models = {\n",
    "        # high numbers of iterations (max_iter) avoids convergence warnings in high-dimensional spaces\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=20000, class_weight=\"balanced\", random_state=random_state), # strong baseline\n",
    "        \"RidgeClassifier\": RidgeClassifier(class_weight=\"balanced\", random_state=random_state), # strong baseline\n",
    "        \"LinearSVC\": LinearSVC(max_iter=20000, class_weight=\"balanced\", random_state=random_state), # strong baseline\n",
    "        \"SVC-RBF\": SVC(probability=False, random_state=random_state, class_weight=\"balanced\"), # probability set to false saves on compute\n",
    "        \"SGD-Hinge\": SGDClassifier(loss=\"hinge\", max_iter=20000, random_state=random_state, class_weight=\"balanced\"), # hinge loss is good for binary classification problems - fast on large data\n",
    "        \"KNN\": KNeighborsClassifier(), # deterministic - sensitive to scaling (good to test with and without standard scaler)\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=random_state, class_weight=\"balanced\"), # don't need scaling and captures nonlinear splits\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=random_state, class_weight=\"balanced\"), # don't need scaling and captures nonlinear splits\n",
    "        \"ExtraTrees\": ExtraTreesClassifier(random_state=random_state, class_weight=\"balanced\"), # don't need scaling and captures nonlinear splits\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(random_state=random_state), # don't need scaling and captures nonlinear splits\n",
    "        \"HistGB\": HistGradientBoostingClassifier(random_state=random_state), # don't need scaling and captures nonlinear splits\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=random_state), # don't need scaling and captures nonlinear splits\n",
    "        \"GaussianNaiveBayes\": GaussianNB(), # deterministic - interpretable\n",
    "    }\n",
    "    return models\n",
    "\n",
    "# get parameter grid to test\n",
    "def small_param_grid(name): # high-leverage parameter sweeps (runtime will be effected if we test everything)\n",
    "    grids = {\n",
    "        # lowered c values due to convergence issues\n",
    "        \"LogisticRegression\": {\"clf__C\": [0.1, 0.5, 1.0]},\n",
    "        \"LinearSVC\":          {\"clf__C\": [0.1, 0.5, 1.0]},\n",
    "        \"SVC-RBF\":            {\"clf__C\": [0.1, 0.5, 1.0], \"clf__gamma\": [\"scale\", \"auto\"]},\n",
    "        \"KNN\":                {\"clf__n_neighbors\": [1, 3, 5, 7, 9, 11, 15, 21, 31], \"clf__weights\": [\"uniform\", \"distance\"], \"clf__metric\": [\"manhattan\", \"euclidean\", \"minkowski\"], \"clf__p\": [1, 2], \"clf__leaf_size\": [10, 20, 30, 40, 50], \"clf__algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]},\n",
    "        \"RandomForest\":       {\"clf__n_estimators\": [300, 600], \"clf__max_depth\": [None, 10, 20]},\n",
    "        \"ExtraTrees\":         {\"clf__n_estimators\": [300, 600], \"clf__max_depth\": [None, 10, 20]},\n",
    "        \"HistGB\":             {\"clf__max_depth\": [None, 6, 10]},\n",
    "    }\n",
    "    return grids.get(name, None)\n",
    "\n",
    "\n",
    "\n",
    "def build_scorers():\n",
    "    return {\n",
    "        \"accuracy\": \"accuracy\", # normal accuracy\n",
    "        \"precision\": make_scorer(precision_score, average=\"binary\", pos_label='bread', zero_division=0),\n",
    "        \"recall\": make_scorer(recall_score, average=\"binary\", pos_label='bread', zero_division=0),\n",
    "        \"f1\": make_scorer(f1_score, average=\"binary\", pos_label='bread', zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ede5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use stratify k-fold as balanced class representation wanted in train and test sets\n",
    "inner_cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=random_state)\n",
    "outer_cv = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=random_state)\n",
    "scorers = build_scorers()\n",
    "\n",
    "results = []\n",
    "# runs process for each model\n",
    "for name, clf in build_models(random_state).items():\n",
    "    # build pipeline\n",
    "    pipe = Pipeline([\n",
    "        (\"l1norm\", Normalizer(norm=\"l1\")),\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    # build parameter grid\n",
    "    grid = small_param_grid(name)\n",
    "\n",
    "    # grid search to find best model parameters\n",
    "    model_for_cv = GridSearchCV(\n",
    "        pipe, grid, scoring=scorers,\n",
    "        cv=inner_cv, n_jobs=N_JOBS, refit=\"accuracy\"\n",
    "    ) if grid else pipe\n",
    "\n",
    "    # cross validate data using best parameters\n",
    "    cv_out = cross_validate(\n",
    "        estimator=model_for_cv,\n",
    "        X=X_train, y=y_train,\n",
    "        cv=outer_cv, scoring=scorers,\n",
    "        return_train_score=False, n_jobs=N_JOBS, return_estimator=True)\n",
    "\n",
    "    # refit on full training set using the same best parameters\n",
    "    model_for_cv.fit(X_train, y_train)\n",
    "\n",
    "    # predictions on holdout\n",
    "    y_pred = model_for_cv.predict(X_test)\n",
    "\n",
    "    if isinstance(model_for_cv, GridSearchCV):\n",
    "        chosen = model_for_cv.best_estimator_\n",
    "        tuned  = model_for_cv.best_params_\n",
    "    else:\n",
    "        chosen = model_for_cv\n",
    "        tuned  = {}\n",
    "\n",
    "    # classifier-only params actually used\n",
    "    clf_params = chosen.named_steps[\"clf\"].get_params()\n",
    "    tuned_json = json.dumps(tuned, default=str)\n",
    "    clf_json   = json.dumps(clf_params, default=str)\n",
    "    \n",
    "    # ROC AUC\n",
    "    y_score = None\n",
    "    if hasattr(model_for_cv, \"predict_proba\"):\n",
    "        y_score = model_for_cv.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model_for_cv, \"decision_function\"):\n",
    "        y_score = model_for_cv.decision_function(X_test)\n",
    "\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"tuned_parameters\": tuned_json,      # only those searched by GridSearchCV\n",
    "        \"clf_parameters\":   clf_json,   \n",
    "        # cv means/stds\n",
    "        **{f\"cv_{k.replace('test_','')}_mean\": float(np.mean(v)) for k, v in cv_out.items() if k.startswith(\"test_\")},\n",
    "        **{f\"cv_{k.replace('test_','')}_std\":  float(np.std(v))  for k, v in cv_out.items() if k.startswith(\"test_\")},\n",
    "\n",
    "        # holdout\n",
    "        \"holdout_accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"holdout_precision\": precision_score(y_test, y_pred, pos_label='bread', zero_division=0),\n",
    "        \"holdout_recall\":    recall_score(y_test, y_pred, pos_label='bread', zero_division=0),\n",
    "        \"holdout_f1\":        f1_score(y_test, y_pred, pos_label='bread', zero_division=0)\n",
    "    }\n",
    "\n",
    "    results.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def best_by_feature_cv(results, metric = 'accuracy', top_n = 3):\n",
    "    col = f\"cv_{metric}_mean\"\n",
    "    return (results\n",
    "            .dropna(subset=[col])\n",
    "            .sort_values([col], ascending=False)\n",
    "            .head(top_n))\n",
    "\n",
    "def best_by_feature_holdout(results, metric = \"accuracy\", top_n = 1):\n",
    "    col = f\"holdout_{metric}\"\n",
    "    return (results\n",
    "            .dropna(subset=[col])\n",
    "            .sort_values([col], ascending=False)\n",
    "            .head(top_n))\n",
    "\n",
    "def overall_best(results, by = \"accuracy\", top_n = 10):\n",
    "    return results.dropna(subset=[f\"holdout_{by}\"]).sort_values(f\"holdout_{by}\", ascending=False).head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ceb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the winners\n",
    "cv_top = best_by_feature_cv(results_df, metric=\"accuracy\", top_n=3)\n",
    "holdout_winners = best_by_feature_holdout(results_df, metric=\"accuracy\", top_n=1)\n",
    "overall_top = overall_best(results_df, by=\"accuracy\", top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d019b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235b38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = build_models(random_state)[list(overall_top['model'])[0]]\n",
    "new_model.set_params(**(json.loads(list(overall_top['clf_parameters'])[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f517843",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(X_train, y_train)\n",
    "y_pred = new_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, pos_label='bread')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, pos_label='bread')}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred, pos_label='bread')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_banana = banana_df.drop(\"label\", axis=1)\n",
    "banana_predicted = new_model.predict(X_banana)\n",
    "print(Counter(banana_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53e930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bananaBread",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
